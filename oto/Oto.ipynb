{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Mobil123 import Mobil123 #class news Scrapper\n",
    "# import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = Detik()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = obj.getIndeksLink([], 1, 'news', 'news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detail = obj.getDetailBerita(links[10:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con = mysql.connector.connect(user='root', password='',\n",
    "#                               host='192.168.43.93',\n",
    "#                               database='news_db')\n",
    "# cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for det in detail:\n",
    "#     add_article = (\"INSERT INTO article\"\n",
    "#               \"(post_id, author, pubdate, category, subcategory, content, comments, images, title, tags, url, source) \"\n",
    "#               \"VALUES (%(id)s, %(author)s, %(pubdate)s, %(category)s, %(subcategory)s, %(content)s, %(comments)s, %(images)s, %(title)s, %(tags)s, %(url)s, %(source)s)\")\n",
    "#     # Insert article\n",
    "#     cursor.execute(add_article, det)\n",
    "#     con.commit()\n",
    "\n",
    "# cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql(\"Select * from article\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes = datetime.now() - timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.strftime(yes, '%a %Y-%b-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetailBerita(self, links):\n",
    "    all_articles = {}\n",
    "    for link in links:\n",
    "        time.sleep(10)\n",
    "        #link\n",
    "        url = link[0]\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "        # Create a BeautifulSoup object from the HTML: soup\n",
    "        soup = BeautifulSoup(html, \"html5lib\")\n",
    "\n",
    "        #extract subcategory from breadcrumb\n",
    "        bc = soup.find('ul', class_=\"breadcrumb\")\n",
    "        if not bc:\n",
    "            continue\n",
    "        \n",
    "        sub = bc.findAll('li')[-2].text\n",
    "        \n",
    "        if (\"foto\" in sub.lower()) or  \"video\" in sub.lower():\n",
    "            continue\n",
    "\n",
    "        #category\n",
    "        articles['category'] = 'Otomotif'\n",
    "        articles['subcategory'] = sub\n",
    "\n",
    "        #article_url\n",
    "        articles['url'] = url\n",
    "\n",
    "        #article\n",
    "        article = soup.find('div', class_=\"content\")\n",
    "\n",
    "        #extract date\n",
    "        scripts = json.loads(soup.findAll('script', {'type':'application/ld+json'})[-1].text)\n",
    "        pubdate = scripts['datePublished']\n",
    "        pubdate = pubdate[0:19].strip(' \\t\\n\\r')\n",
    "        articles['pubdate'] = datetime.strftime(datetime.strptime(pubdate, \"%Y-%m-%dT%H:%M:%S\"), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        #articleid\n",
    "        articleid = url.replace('/','')\n",
    "        articleid = url.split('-')\n",
    "        articleid = int(articleid[-1][-5:])\n",
    "        articles['id'] = articleid\n",
    "        \n",
    "        #extract editor\n",
    "        author = soup.find('div', class_=\"publish-cont\").find('a').text\n",
    "        articles['author'] = author\n",
    "\n",
    "        #extract title\n",
    "        title = soup.find('article', class_=\"newslistouter container-base\").find('h1').text\n",
    "        articles['title'] = title\n",
    "\n",
    "        #source\n",
    "        articles['source'] = 'oto.com'\n",
    "\n",
    "        #extract comments count\n",
    "        articles['comments'] = 0     \n",
    "\n",
    "        #extract tags\n",
    "        #tags = soup.find('meta', attrs={\"property\":\"article:tag\"})['content']\n",
    "        #articles['tags'] = ','.join([x.text for x in tags])\n",
    "\n",
    "        #extract images\n",
    "        image = article.find('img')['src']\n",
    "        articles['image'] = image\n",
    "\n",
    "        #hapus link sisip\n",
    "        for link in article.findAll('img'):\n",
    "            link.decompose()\n",
    "\n",
    "        for link in article.findAll('div'):\n",
    "            link.decompose()\n",
    "    \n",
    "        #extract content\n",
    "        detail = BeautifulSoup(article.decode_contents().replace('<br/>', ' '), \"html5lib\")\n",
    "        content = re.sub(r'\\n|\\t|\\b|\\r','',detail.text)\n",
    "        articles['content']\n",
    "        #print('memasukkan berita id ', articles['id'])\n",
    "        all_articles.append(articles)\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndeksLink(self, links, page, cat, date=datetime.strftime(datetime.today(), '%Y/%m/%d')):\n",
    "    \"\"\"\n",
    "    Untuk mengambil seluruh url\n",
    "    link pada indeks category tertentu\n",
    "    date format : YYYY/mm/dd\n",
    "    category : berita-mobil, artikel-mobil, uncategorized\n",
    "    \"\"\"\n",
    "    print(\"page \", page)\n",
    "    url = \"https://www.otomart.id/berita/category/\"+cat\n",
    "    url = \"https://www.mobil123.com/berita/\"+cat+\"?page_number=\"+str(page)\n",
    "    print(url)\n",
    "\n",
    "    # Make the request and create the response object: response\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except ConnectionError:\n",
    "        print(\"Connection Error, but it's still trying...\")\n",
    "        time.sleep(10)\n",
    "        links = self.getIndeksLink(links, page+1, cat, category, date)\n",
    "    # Extract HTML texts contained in Response object: html\n",
    "    html = response.text\n",
    "    # Create a BeautifulSoup object from the HTML: soup\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    "\n",
    "    contentDiv = soup.find('li', class_=\"card\")\n",
    "    if contentDiv:\n",
    "        for post in contentDiv.findAll('a'):\n",
    "            link = [post.find('a', href=True)['href'], cat]\n",
    "            links.append(link)\n",
    "\n",
    "    max_page = math.ceil((int(soup.find('div', class_=\"news-count\").find('span').text))/12)\n",
    "\n",
    "        if page <= max_page:\n",
    "            time.sleep(10)\n",
    "            links = self.getIndeksLink(links, page+1, cat, date)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'ID')\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.oto.com/berita-mobil/giias-2018-jaring-penjualan-lebih-suzuki-tebar-promo-21181719\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "# Extract HTML texts contained in Response object: html\n",
    "html = response.text\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-c67f50b6259f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpubdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpubdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpubdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' \\t\\n\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0marticles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pubdate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpubdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%Y-%m-%dT%H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'articles' is not defined"
     ]
    }
   ],
   "source": [
    "scripts = json.loads(soup.findAll('script', {'type':'application/ld+json'})[-1].text)\n",
    "pubdate = scripts['datePublished']\n",
    "pubdate = pubdate[0:19].strip(' \\t\\n\\r')\n",
    "articles['pubdate'] = datetime.strftime(datetime.strptime(pubdate, \"%Y-%m-%dT%H:%M:%S\"), '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
