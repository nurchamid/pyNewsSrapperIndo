{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Oto import Oto #class news Scrapper\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Oto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page  1\n",
      "https://www.oto.com/berita-motor?page1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-d8332fc0aeb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetIndeksLink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'berita-motor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\OJT\\pyNewsSrapperIndo\\oto\\Oto.py\u001b[0m in \u001b[0;36mgetIndeksLink\u001b[1;34m(self, links, page, cat, date)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindeks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[1;31m#check if there are a post with same url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"SELECT count(*) FROM article WHERE url like '\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "links = obj.getIndeksLink([], 1, 'berita-motor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detail = obj.getDetailBerita(links[10:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con = mysql.connector.connect(user='root', password='',\n",
    "#                               host='192.168.43.93',\n",
    "#                               database='news_db')\n",
    "# cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for det in detail:\n",
    "#     add_article = (\"INSERT INTO article\"\n",
    "#               \"(post_id, author, pubdate, category, subcategory, content, comments, images, title, tags, url, source) \"\n",
    "#               \"VALUES (%(id)s, %(author)s, %(pubdate)s, %(category)s, %(subcategory)s, %(content)s, %(comments)s, %(images)s, %(title)s, %(tags)s, %(url)s, %(source)s)\")\n",
    "#     # Insert article\n",
    "#     cursor.execute(add_article, det)\n",
    "#     con.commit()\n",
    "\n",
    "# cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql(\"Select * from article\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes = datetime.now() - timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.strftime(yes, '%a %Y-%b-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetailBerita(self, links):\n",
    "    all_articles = {}\n",
    "    for link in links:\n",
    "        time.sleep(10)\n",
    "        #link\n",
    "        url = link[0]\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "        # Create a BeautifulSoup object from the HTML: soup\n",
    "        soup = BeautifulSoup(html, \"html5lib\")\n",
    "\n",
    "        #extract subcategory from breadcrumb\n",
    "        bc = soup.find('ul', class_=\"breadcrumb\")\n",
    "        if not bc:\n",
    "            continue\n",
    "        \n",
    "        sub = bc.findAll('li')[-2].text\n",
    "        \n",
    "        if (\"foto\" in sub.lower()) or  \"video\" in sub.lower():\n",
    "            continue\n",
    "\n",
    "        #category\n",
    "        articles['category'] = 'Otomotif'\n",
    "        articles['subcategory'] = sub\n",
    "\n",
    "        #article_url\n",
    "        articles['url'] = url\n",
    "\n",
    "        #article\n",
    "        article = soup.find('div', class_=\"content\")\n",
    "\n",
    "        #extract date\n",
    "        scripts = json.loads(soup.findAll('script', {'type':'application/ld+json'})[-1].text)\n",
    "        pubdate = scripts['datePublished']\n",
    "        pubdate = pubdate[0:19].strip(' \\t\\n\\r')\n",
    "        articles['pubdate'] = datetime.strftime(datetime.strptime(pubdate, \"%Y-%m-%dT%H:%M:%S\"), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        #articleid\n",
    "        articleid = url.replace('/','')\n",
    "        articleid = url.split('-')\n",
    "        articleid = int(articleid[-1][-5:])\n",
    "        articles['id'] = articleid\n",
    "        \n",
    "        #extract editor\n",
    "        author = soup.find('div', class_=\"publish-cont\").find('a').text\n",
    "        articles['author'] = author\n",
    "\n",
    "        #extract title\n",
    "        title = soup.find('article', class_=\"newslistouter container-base\").find('h1').text\n",
    "        articles['title'] = title\n",
    "\n",
    "        #source\n",
    "        articles['source'] = 'oto.com'\n",
    "\n",
    "        #extract comments count\n",
    "        articles['comments'] = 0     \n",
    "\n",
    "        #extract tags\n",
    "        #tags = soup.find('meta', attrs={\"property\":\"article:tag\"})['content']\n",
    "        #articles['tags'] = ','.join([x.text for x in tags])\n",
    "\n",
    "        #extract images\n",
    "        image = article.find('img')['src']\n",
    "        articles['image'] = image\n",
    "\n",
    "        #hapus link sisip\n",
    "        for link in article.findAll('img'):\n",
    "            link.decompose()\n",
    "\n",
    "        for link in article.findAll('div'):\n",
    "            link.decompose()\n",
    "    \n",
    "        #extract content\n",
    "        detail = BeautifulSoup(article.decode_contents().replace('<br/>', ' '), \"html5lib\")\n",
    "        content = re.sub(r'\\n|\\t|\\b|\\r','',detail.text)\n",
    "        articles['content']\n",
    "        #print('memasukkan berita id ', articles['id'])\n",
    "        all_articles.append(articles)\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndeksLink(self, links, page, cat, date=datetime.strftime(datetime.today(), '%Y/%m/%d')):\n",
    "    \"\"\"\n",
    "    Untuk mengambil seluruh url\n",
    "    link pada indeks category tertentu\n",
    "    date format : YYYY/mm/dd\n",
    "    category : berita-mobil, artikel-mobil, uncategorized\n",
    "    \"\"\"\n",
    "    print(\"page \", page)\n",
    "    url = \"https://www.otomart.id/berita/category/\"+cat\n",
    "    url = \"https://www.mobil123.com/berita/\"+cat+\"?page_number=\"+str(page)\n",
    "    print(url)\n",
    "\n",
    "    # Make the request and create the response object: response\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except ConnectionError:\n",
    "        print(\"Connection Error, but it's still trying...\")\n",
    "        time.sleep(10)\n",
    "        links = self.getIndeksLink(links, page+1, cat, category, date)\n",
    "    # Extract HTML texts contained in Response object: html\n",
    "    html = response.text\n",
    "    # Create a BeautifulSoup object from the HTML: soup\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    "\n",
    "    contentDiv = soup.find('li', class_=\"card\")\n",
    "    if contentDiv:\n",
    "        for post in contentDiv.findAll('a'):\n",
    "            link = [post.find('a', href=True)['href'], cat]\n",
    "            links.append(link)\n",
    "\n",
    "    max_page = math.ceil((int(soup.find('div', class_=\"news-count\").find('span').text))/12)\n",
    "\n",
    "        if page <= max_page:\n",
    "            time.sleep(10)\n",
    "            links = self.getIndeksLink(links, page+1, cat, date)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'ID')\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.oto.com/berita-mobil?page2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'berita-mobil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "# Extract HTML texts contained in Response object: html\n",
    "html = response.text\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeks = soup.findAll('li', class_=\"card\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeks = soup.findAll('li', class_=\"card\")\n",
    "for post in indeks:\n",
    "    link = [post.find('a', href=True)['href'], 'cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.oto.com/berita-mobil/giias-2018-mazda-cx-3-facelift-ini-daftar-ubahannya-21181852',\n",
       " 'cat']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
