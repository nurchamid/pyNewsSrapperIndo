{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1= 'http://www.tribunnews.com/regional/2018/01/01/ust-atm-memuji-kegiatan-keagamaan-rutin-yang-dilakukan-pemkot-makassar?page=all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.remote.webdriver import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Remote(\n",
    "desired_capabilities=webdriver.DesiredCapabilities.HTMLUNITWITHJS )\n",
    "driver.get('http://www.google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'ID')\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from requests.exceptions import ConnectionError\n",
    "import unicodedata\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://travel.detik.com/detiktravel/d-3876394/fakta-fakta-tentang-kesakralan-tempat-ibadah-pura-bali'\n",
    "articles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://travel.detik.com/detiktravel/d-3876394/fakta-fakta-tentang-kesakralan-tempat-ibadah-pura-bali\n",
      "tes\n",
      "err detail\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-af8e48eaf0b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'err detail'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m#hapus link sisip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mdetail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"linksisip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdetail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"linksisip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "print(url)\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "except ConnectionError:\n",
    "    print(\"Connection Error, but it's still trying...\")\n",
    "    time.sleep(10)\n",
    "    self.getDetailBerita(link)\n",
    "except:\n",
    "    print('err koneksi')\n",
    "html = response.text\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html, \"html5lib\")\n",
    "# print(soup)\n",
    "#extract subcategory from breadcrumb\n",
    "bc = soup.find('div', class_=\"breadcrumb\")\n",
    "if not bc:\n",
    "    print('err bc')\n",
    "\n",
    "sub = bc.findAll('a')[1].get_text(strip=True)\n",
    "if (\"foto\" in sub.lower()) or (\"detiktv\" in sub.lower()) or (\"video\" in sub.lower()) or (\"photos\" in sub.lower()) or (\"videos\" in sub.lower()):\n",
    "    print('err foto video')\n",
    "\n",
    "articles['subcategory'] = sub\n",
    "#category\n",
    "articles['category'] = 'tes'\n",
    "articles['url'] = url\n",
    "\n",
    "article = soup.find('article')\n",
    "\n",
    "#extract date\n",
    "pubdate = soup.find(\"meta\", attrs={'name':'publishdate'})\n",
    "if pubdate:\n",
    "    pubdate = pubdate['content'].strip(' \\t\\n\\r')\n",
    "    articles['pubdate'] = datetime.strftime(datetime.strptime(pubdate, \"%Y/%m/%d %H:%M:%S\"), '%Y-%m-%d %H:%M:%S')\n",
    "    id = soup.find(\"meta\", attrs={'name':'articleid'})\n",
    "    articles['id'] = int(id['content']) if id else int(datetime.strptime(pubdate, \"%Y/%m/%d %H:%M:%S\").timestamp()) + len(url)\n",
    "else:\n",
    "    pubdate = soup.find('span', {'class':'date'})\n",
    "    pubdate = pubdate.get_text(strip=True).strip(' \\t\\n\\r').replace(\" WIB\", '')\n",
    "    articles['pubdate'] = datetime.strftime(datetime.strptime(pubdate, \"%A, %d %b %Y %H:%M\"), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    id = soup.find(\"meta\", attrs={'name':'articleid'})\n",
    "    articles['id'] = int(id['content']) if id else int(datetime.strptime(pubdate, \"%A, %d %b %Y %H:%M\").timestamp()) + len(url)\n",
    "\n",
    "#extract author\n",
    "author = soup.find(\"meta\", attrs={'name':'author'})\n",
    "articles['author'] = author['content'] if author else ''\n",
    "\n",
    "#extract title\n",
    "title =  article.find('meta', {\"property\":\"og:title\"})\n",
    "articles['title'] = title.get_text(strip=True) if title else ''\n",
    "\n",
    "#source\n",
    "articles['source'] = 'detik'\n",
    "\n",
    "#extract comments count\n",
    "komentar = soup.find('a', class_=\"komentar\")\n",
    "articles['comments'] = int(komentar.find('span').get_text(strip=True).replace('Komentar', '').strip(' \\t\\n\\r')) if komentar else 0\n",
    "\n",
    "#extract tags\n",
    "tags = article.find('div', class_=\"detail_tag\")\n",
    "articles['tags'] = ','.join([x.get_text(strip=True) for x in tags.findAll('a')]) if tags else ''\n",
    "\n",
    "#extract images\n",
    "images = article.find('div', class_=\"pic_artikel\")\n",
    "articles['images'] = images.find('img')['src'] if images else ''\n",
    "print(articles['category'])\n",
    "#extract detail\n",
    "if articles['category'] == 'news':\n",
    "    detail = article.find('div', class_=\"detail_text\")\n",
    "else:\n",
    "    detail = article.find('div', attrs={\"id\": \"detikdetailtext\"})\n",
    "    if not de\n",
    "if not detail:\n",
    "    print('err detail')\n",
    "#hapus link sisip\n",
    "if detail.findAll('table', class_=\"linksisip\"):\n",
    "    for link in detail.findAll('table', class_=\"linksisip\"):\n",
    "        link.decompose()\n",
    "\n",
    "#hapus video sisip\n",
    "if detail.findAll('div', class_=\"sisip_embed_sosmed\"):\n",
    "    for tag in detail.findAll('div', class_=\"sisip_embed_sosmed\"):\n",
    "        tag.decompose()\n",
    "\n",
    "#hapus all setelah clear fix\n",
    "if detail.find('div', class_=\"clearfix mb20\"):\n",
    "    for det in detail.find('div', class_=\"clearfix mb20\").findAllNext():\n",
    "        det.decompose()\n",
    "\n",
    "#hapus all script\n",
    "for script in detail.findAll('script'):\n",
    "    script.decompose()\n",
    "\n",
    "for p in detail.findAll('p'):\n",
    "    if (\"baca juga\" in p.get_text(strip=True).lower()) and (p.find('a')):\n",
    "        p.decompose()\n",
    "\n",
    "#extract content\n",
    "detail = BeautifulSoup(detail.decode_contents().replace('<br/>', ' '), \"html5lib\")\n",
    "content = re.sub(r'\\n|\\t|\\b|\\r','',unicodedata.normalize(\"NFKD\",detail.get_text(strip=True)))\n",
    "articles['content'] = re.sub(r'(Tonton juga).*','', content)\n",
    "print('memasukkan berita id ', articles['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.viva.co.id/indeks/all/all/2018/01/01?type=art\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.viva.co.id/indeks/all/all/2018/01/01?type=art\"\n",
    "print(url)\n",
    "#scarp with selenium\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')  # Last I checked this was necessary.\n",
    "options.add_argument('--disable-extensions')\n",
    "\n",
    "driver = webdriver.Chrome(\"./chromedriver.exe\", chrome_options=options)\n",
    "try:\n",
    "    driver.get(url)\n",
    "except ConnectionError:\n",
    "    print(\"Connection Error, but it's still trying...\")\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "tes = soup.find('ul', attrs={'id': 'load_terbaru_content'})\n",
    "lel = re.sub(r'\\n|\\t|\\b|\\r|\\s','',tes.findAll('script', attrs={\"type\":\"text/javascript\"})[-1].get_text(strip=True)).strip(' \\t\\n\\r')\n",
    "start = len(lel[lel.find('window.last_publish_date'):lel.find('window.last_publish_date')+26])+lel.find('window.last_publish_date')\n",
    "end = start + 19\n",
    "date_max = datetime.strftime(datetime.strptime(lel[start:end], \"%Y-%m-%d %H:%M:%S\"), '%Y-%m-%d')\n",
    "batas_el = tes.findAll('li', class_=\"content_center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batas_el:\n",
    "    li = batas_el[-1].findAllNext('li')\n",
    "else:\n",
    "    li = tes.findAll('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n"
     ]
    }
   ],
   "source": [
    "for post in li:\n",
    "    if post.find('div', class_=\"date\") :\n",
    "        date_post = post.find('div', class_=\"date\").get_text(strip=True).split(' | ')[0]\n",
    "        date_post = datetime.strftime(datetime.strptime(date_post, \"%d %B %Y\"), '%Y/%m/%d')\n",
    "        print(date_post)\n",
    "#         if date_post == \"2018/01/01\":\n",
    "#             link = [post.find('a', class_=\"title-content\", href=True)['href'], \"\"]\n",
    "#             print('masukan link ', link[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
