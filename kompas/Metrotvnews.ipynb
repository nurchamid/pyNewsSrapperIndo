{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'ID')\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET INDEKS LINK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndeksLink(links, page, cat_link, category, date=datetime.strftime(datetime.today(), '%Y-%m-%d')):\n",
    "    \"\"\"\n",
    "    Untuk mengambil seluruh url \n",
    "    link pada indeks category tertentu\n",
    "    date format : YYYY-mm-dd\n",
    "    \"\"\"\n",
    "    print(\"page \", page)\n",
    "    if cat_link == 'otomotif':\n",
    "        url = \"https://indeks.kompas.com/\"+cat_link+\"/\"+date+\"/\"+str(page)\n",
    "    else :\n",
    "        url = \"https://indeks.kompas.com/\"+cat_link+\"/\"+date+\"/\"+str(page)\n",
    "    print(url)\n",
    "    # Make the request and create the response object: response\n",
    "    response = requests.get(url)\n",
    "    # Extract HTML texts contained in Response object: html\n",
    "    html = response.text\n",
    "    # Create a BeautifulSoup object from the HTML: soup\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    "    contentDiv = soup.findAll('div', class_=\"latest--indeks mt2 clearfix\")\n",
    "    for post in contentDiv:\n",
    "        link = [post.find('a', href=True)['href'], category]\n",
    "        links.append(link)\n",
    "        \n",
    "    el_page = soup.find('div', class_=\"paging__wrap clearfix\")\n",
    "    if el_page:\n",
    "        a_page = soup.find('div', class_=\"paging__wrap clearfix\").findAll('div', class_='paging__item')[-1].find('a')\n",
    "        if 'data-ci-pagination-page' in a_page:\n",
    "            max_page = page\n",
    "        else:\n",
    "            max_page = int(a_page['data-ci-pagination-page'].replace('\\n', '').strip(' '))\n",
    "            \n",
    "        if page < max_page:\n",
    "            links = getIndeksLink(links, page+1, cat_link, category, date)\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET DETAIL BERITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetailBerita(links):\n",
    "    all_articles = []\n",
    "    for link in links:\n",
    "        articles = {}\n",
    "        #link\n",
    "        url = link[0]\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "        \n",
    "        # Create a BeautifulSoup object from the HTML: soup\n",
    "        soup = BeautifulSoup(html, \"html5lib\")\n",
    "        \n",
    "        #extract subcategory from breadcrumb\n",
    "        bc = soup.find('div', class_=\"breadcrumbs\")\n",
    "        if not bc:\n",
    "            continue\n",
    "        \n",
    "        sub = bc.findAll('a')[1].text\n",
    "        if (\"foto\" in sub.lower()) or  \"video\" in sub.lower(): \n",
    "            continue\n",
    "            \n",
    "        articles['subcategory'] = sub\n",
    "        \n",
    "        #articles['id'] = int(soup.find(\"meta\", attrs={'name':'articleid'})['content'])\n",
    "        \n",
    "        #category\n",
    "        articles['category'] = link[1]\n",
    "        articles['url'] = url\n",
    "        \n",
    "        article = soup.find('div', class_=\"tru\")\n",
    "        \n",
    "        #extract date\n",
    "#         pubdate = soup.find(\"meta\", attrs={'name':'publishdate'})['content']\n",
    "#         pubdate = pubdate.strip(' \\t\\n\\r')\n",
    "#         articles['pubdate'] = datetime.strftime(datetime.strptime(pubdate, \"%Y/%m/%d %H:%M:%S\"), '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        #extract author\n",
    "#         articles['author'] = soup.find(\"meta\", attrs={'name':'author'})['content']\n",
    "        \n",
    "        #extract title\n",
    "        articles['title'] = soup.find('meta', attrs={\"property\":\"og:title\"})['content']\n",
    "        \n",
    "        #source\n",
    "        articles['source'] = 'metrotvnews'\n",
    "        \n",
    "        #extract comments count\n",
    "        #articles['comments'] = int(soup.find('a', class_=\"komentar\").find('span').text.replace('Komentar', '').strip(' \\t\\n\\r'))\n",
    "        \n",
    "        #extract tags\n",
    "        tags = soup.find('div', class_=\"line\").findAll('a', class_=\"tag\")\n",
    "        articles['tags'] = ','.join([x.text for x in tags])\n",
    "        \n",
    "        #extract images\n",
    "        articles['images'] = soup.find('img', class_=\"pic\")['src']\n",
    "        \n",
    "        #extract detail\n",
    "        detail = soup.find('div', class_=\"tru\")\n",
    "        \n",
    "        #hapus link sisip\n",
    "        for link in detail.findAll('div', class_=\"related extra\"):\n",
    "            link.decompose()\n",
    "        \n",
    "#         #hapus video sisip\n",
    "#         for tag in detail.findAll('div', class_=\"sisip_embed_sosmed\"):\n",
    "#             tag.decompose()\n",
    "            \n",
    "#         #hapus all setelah clear fix\n",
    "#         for det in detail.find('div', class_=\"clearfix mb20\").findAllNext():\n",
    "#             det.decompose()\n",
    "            \n",
    "#         #hapus all script\n",
    "#         for script in detail.findAll('script'):\n",
    "#             script.decompose()\n",
    "        \n",
    "        #extract content\n",
    "#         detail = BeautifulSoup(detail.decode_contents().replace('<br/>', ' '), \"html5lib\")\n",
    "#         content = re.sub(r'\\n|\\t|\\b|\\r','',detail.text)\n",
    "#         articles['content'] = re.sub(r'(Tonton juga).*','', content)\n",
    "#         print('memasukkan berita id ', articles['id'])\n",
    "#         all_articles.append(articles)\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://otomotif.metrotvnews.com/modifikasi/GNlAap2b-the-black-bison-glc-class-dari-tanah-tiongkok\")\n",
    "    # Extract HTML texts contained in Response object: html\n",
    "html = response.text\n",
    "    # Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles['id'] = int(soup.find(\"meta\", attrs={'name':'cXenseParse:articleid'})['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modifikasi'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = {}\n",
    "bc = soup.find('div', class_=\"breadcrumbs\")\n",
    "articles['subcategory'] = bc.findAll('a')[1].text\n",
    "articles['subcategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = soup.find(\"div\", class_=\"page\")\n",
    "# article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M. Bagus Rachmanto \\xa0\\xa0 • \\xa0\\xa0 Sabtu, 28 Jul 2018 12:25 WIB'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubdate = soup.find(\"div\",class_=\"reg\").text\n",
    "pubdate = pubdate.strip(' \\t\\n\\r')\n",
    "pubdate = pubdate\n",
    "pubdate\n",
    "# pubdate = pubdate.replace()\n",
    "# articles['pubdate'] = datetime.strftime(datetime.strptime(pubdate, \"%Y/%m/%d %H:%M:%S\"), '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonBreakSpace = '\\xa0'\n",
    "pubdate = pubdate.replace(nonBreakSpace, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['M. Bagus Rachmanto  •  Sabtu, 28 Jul 2018 12:25 WIB']]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubdate = [pubdate]\n",
    "pubdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M. Bagus Rachmanto  •  Sabtu, 28 Jul 2018 12:25 WIB']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubdate[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Black Bison GLC-Class dari Tanah Tiongkok'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['title'] = soup.find('meta', attrs={\"property\":\"og:title\"})['content']\n",
    "articles['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles['comments'] = int(soup.find('a', class_=\"komentar\").find('span').text.replace('Komentar', '').strip(' \\t\\n\\r'))\n",
    "# articles['comments'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modifikasi mobil'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = soup.find('div', class_=\"line\").findAll('a', class_=\"tag\")\n",
    "articles['tags'] = ','.join([x.text for x in tags])\n",
    "articles['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://cdn.metrotvnews.com/dynamic/content/2018/07/28/907819/Zgx2Qm96lt.jpg?w=650'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['images'] = soup.find('img', class_=\"pic\")['src']\n",
    "articles['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = article.find('div', class_=\"detail_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"tru\">\n",
       "            <p>\n",
       "            <strong>Jepang: </strong>Tuner asal Jepang, Wald International, punya cara tersendiri dalam melakukan modifikasi dan memiliki tampilan khusus yang berlaku untuk semua <em>body kit</em>-nya, sehigga sangat mudah diprediksi kalau itu adalah hasil garapanya.<br/><br/>\n",
       "Hal itu terlihat dari hasil modifikasi yang mereka lakukan terhadapp SUV Mercedes pertama mereka, yakni dengan mengubah tampilan SUV asal Jerman itu seperti GLC-Class dan dijuluki The Black Bison GLC-Class.<br/>\n",
       "<img alt=\"\" class=\"big_center fluid\" src=\"http://cdn.medcom.id/images/library/images/oto/2018/07%20Juli/GLC3.jpg\"/><br/>\n",
       "Ciri khas modifikasi hasil karya Wald International terlihat pada bagian bumper baru, dengan  memasangkan kisi-kisi yang direvisi dan spoiler dagu yang terinspirasi F1 yang dipasang di bagian tengah. \n",
       " \n",
       "\n",
       "    </p><div class=\"related\">\n",
       "        <div class=\"ti3 ani cl-effect-4\">\n",
       "            <a href=\"http://otomotif.metrotvnews.com/topic/5808\">Baca juga</a>\n",
       "        </div>\n",
       "        \n",
       "        <ul>\n",
       "               \n",
       "            <li><a href=\"http://otomotif.metrotvnews.com/modifikasi/yNLdllPN-auto-dynamic-suntik-tenaga-bmw-x5-jadi-lebih-sangar\">Auto Dynamic Suntik Tenaga BMW X5 Jadi Lebih Sangar</a></li>\n",
       "        \n",
       "               \n",
       "            <li><a href=\"http://otomotif.metrotvnews.com/modifikasi/zNAwDwvk-nissan-gt-r-jadi-jadian-ini-berbasis-daihatsu-copen\">Nissan GT-R Jadi-Jadian ini Berbasis Daihatsu Copen</a></li>\n",
       "        \n",
       "               \n",
       "            <li><a href=\"http://otomotif.metrotvnews.com/modifikasi/yKX9oJ6N-modifikasi-audi-rs6-bergaya-army\">Modifikasi Audi RS6 Bergaya Army</a></li>\n",
       "        \n",
       "                </ul>\n",
       "    </div>\n",
       "<p></p>\n",
       "\n",
       "Meski belum diuji apakah pemasangan kisi-kisi bergaya F1 itu memberikan downforce seperti di mobil balap, tetapi itu bukanlah masalah, karena pemasangannya membuat tampilan mobil menjadi lebih keren.<br/>\n",
       "<img alt=\"\" class=\"big_center fluid\" src=\"http://cdn.medcom.id/images/library/images/oto/2018/07%20Juli/GLC2.jpg\"/><br/>\n",
       "Wald International juga melengkapi SUV dengan grill Panamericana, khusus untuk model GLC 63 yang pemasangannya plug in ke model biasa. Termasuk pemasangan side skirts dan apron belakang, satu set tips quad knal square dan lampu stop baru. Tak ketinggalan pemilihan pelek baru yang disesuaikan dengan tongkrongan GLC, dan pemasangan kaca film buatan Wald Great China. <br/><br/>\n",
       "Seperti diketahui, saat ini Brabus adalah rumah modifikasi asal Jerman yang khusus menggarap model Mercedes-Benz, apa yang dilakuakan Wald international setidaknya memberikan warna baru dan alternatif pilihan akan modifikasi terhadap mmobil pabrikan asal Jerman tersebut.<br/><br/>            <br/>(UDA)            \n",
       "            <p></p>\n",
       "\n",
       "            <br/><script type=\"text/javascript\">\n",
       "                        OA_show('otomotif_signature');\n",
       "                    </script>        </div>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail = soup.find('div', class_=\"tru\")\n",
    "detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hapus link sisip\n",
    "for link in detail.findAll('div', class_=\"related extra\"):\n",
    "    link.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = \"http://otomotif.metrotvnews.com/modifikasi/GNlAap2b-the-black-bison-glc-class-dari-tanah-tiongkok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getDetailBerita(\"http://otomotif.metrotvnews.com/modifikasi/GNlAap2b-the-black-bison-glc-class-dari-tanah-tiongkok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
